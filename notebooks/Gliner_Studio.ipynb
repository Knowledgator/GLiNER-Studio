{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeBDnZK9KHcz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Installations\n",
        "# !pip install gradio gliner\n",
        "# !pip install accelerate -U\n",
        "# !pip install transformers huggingface_hub\n",
        "\n",
        "import gradio as gr\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "import torch\n",
        "from gliner import GLiNER\n",
        "from gliner import GLiNERConfig, GLiNER\n",
        "from gliner.training import Trainer, TrainingArguments\n",
        "from gliner.data_processing.collator import DataCollatorWithPadding\n",
        "from gliner.utils import load_config_as_namespace\n",
        "from gliner.data_processing import WordsSplitter, GLiNERDataset\n",
        "\n",
        "if not os.path.exists(\"models\"):\n",
        "        os.makedirs(\"models\")\n",
        "if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "\n",
        "# List of available models\n",
        "AVAILABLE_MODELS = [\n",
        "    \"knowledgator/gliner-multitask-large-v0.5\",\n",
        "    \"urchade/gliner_multi-v2.1\",\n",
        "    \"urchade/gliner_large_bio-v0.1\",\n",
        "    \"numind/NuNER_Zero\",\n",
        "    \"EmergentMethods/gliner_medium_news-v2.1\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title #Upload your sentences examples\n",
        "import gradio as gr\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Ensure the /data directory exists\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Function to save the uploaded file\n",
        "def save_file(uploaded_file):\n",
        "    if uploaded_file is None:\n",
        "        return \"No file uploaded.\"\n",
        "\n",
        "    # Define the path where the file will be saved\n",
        "    save_path = os.path.join(\"data\")\n",
        "\n",
        "    try:\n",
        "        # Save the file with the new name\n",
        "        shutil.copy(uploaded_file.name, save_path)\n",
        "        return f\"File saved to {save_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as loader:\n",
        "    gr.Markdown(\"# File Upload and Save Example\")\n",
        "\n",
        "    # File uploader component\n",
        "    file_uploader = gr.File(label=\"Upload your file here\")\n",
        "\n",
        "    # Button to trigger the file save function\n",
        "    save_button = gr.Button(\"Save File\")\n",
        "\n",
        "    # Output textbox to show the result\n",
        "    output = gr.Textbox(label=\"Result\")\n",
        "\n",
        "    # Link the button to the save_file function\n",
        "    save_button.click(fn=save_file, inputs=file_uploader, outputs=output)\n",
        "\n",
        "# Launch the interface\n",
        "loader.launch(share=True, inline=True)\n"
      ],
      "metadata": {
        "id": "cBUCEExkKOlF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you don't have the final dataset, upload sentences examples to auto annotate them, otherwise, upload your file and skip until validation.**\n",
        "\n",
        "**Run the cell above ☝️**\n",
        "\n",
        "**Or you can write your custom function for loading of a dataset 👇**\n",
        "\\\n",
        "\\\n",
        "**🛑 If you have already annotated dataset, please scroll down, there is a way to load it directly 🛑**\n",
        "\n"
      ],
      "metadata": {
        "id": "mgjpcmnJKf0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a function to transform your data to the list of sentences\n",
        "from typing import List\n",
        "\n",
        "def fc() -> List[str]:\n",
        "  pass\n",
        "\n",
        "#DO NOT CHANGE THE NAME OF VARIABLE\n",
        "sentences = fc()"
      ],
      "metadata": {
        "id": "4CHXnMkKukaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "def fc(filename):\n",
        "  with open(filename, 'r') as f:\n",
        "    sentences = f.readlines()\n",
        "  return sentences\n",
        "\n",
        "sentences = fc('data/test.txt')\n",
        "sentences"
      ],
      "metadata": {
        "id": "QcDyilpMMXt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "sentences = [\n",
        "        \"IBM Watson defeated human champions in the game of Jeopardy!\",\n",
        "        \"The Amazon rainforest is known as the lungs of the Earth.\",\n",
        "        \"Sydney Opera House is an iconic symbol of Australia.\",\n",
        "        \"A journey of a thousand miles begins with a single step.\",\n",
        "        \"Google is building a new office in New York.\",\n",
        "        \"The movie Inception was directed by Christopher Nolan.\"\n",
        "        \"Jeff Bezos founded Amazon in 1994.\",\n",
        "    ]"
      ],
      "metadata": {
        "id": "GJ99KyV11MOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prepare Data for Manual Annotation\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenize the input text into a list of tokens.\"\"\"\n",
        "    return re.findall(r'\\w+(?:[-_]\\w+)*|\\S', text)\n",
        "\n",
        "def prepare_data_for_manual_annotation(sentences):\n",
        "  annotated_data = []\n",
        "  for text in sentences:\n",
        "    annotated_data.append({\"tokenized_text\": tokenize_text(text), \"ner\": [], \"validated\": False})\n",
        "  with open(\"data/annotated_data.json\", \"wt\") as file:\n",
        "    json.dump(annotated_data, file)\n",
        "\n",
        "prepare_data_for_manual_annotation(sentences)\n"
      ],
      "metadata": {
        "id": "BbhM8OO3MNI0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Auto Annotation\n",
        "\n",
        "# Provided post-processing functions\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenize the input text into a list of tokens.\"\"\"\n",
        "    return re.findall(r'\\w+(?:[-_]\\w+)*|\\S', text)\n",
        "\n",
        "def transform_data(data):\n",
        "    tokens = tokenize_text(data['text'])\n",
        "    spans = []\n",
        "\n",
        "    for entity in data['entities']:\n",
        "        entity_tokens = tokenize_text(entity['word'])\n",
        "        entity_length = len(entity_tokens)\n",
        "\n",
        "        # Find the start and end indices of each entity in the tokenized text\n",
        "        for i in range(len(tokens) - entity_length + 1):\n",
        "            if tokens[i:i + entity_length] == entity_tokens:\n",
        "                spans.append([i, i + entity_length - 1, entity['entity']])\n",
        "                break\n",
        "\n",
        "    return {\"tokenized_text\": tokens, \"ner\": spans, \"validated\": False}\n",
        "\n",
        "def merge_entities(entities):\n",
        "    if not entities:\n",
        "        return []\n",
        "    merged = []\n",
        "    current = entities[0]\n",
        "    for next_entity in entities[1:]:\n",
        "        if next_entity['entity'] == current['entity'] and (next_entity['start'] == current['end'] + 1 or next_entity['start'] == current['end']):\n",
        "            current['word'] += ' ' + next_entity['word']\n",
        "            current['end'] = next_entity['end']\n",
        "        else:\n",
        "            merged.append(current)\n",
        "            current = next_entity\n",
        "    merged.append(current)\n",
        "    return merged\n",
        "\n",
        "def annotate_text(\n",
        "    model, text, labels: List[str], threshold: float, nested_ner: bool\n",
        ") -> Dict:\n",
        "    labels = [label.strip() for label in labels]\n",
        "    r = {\n",
        "        \"text\": text,\n",
        "        \"entities\": [\n",
        "            {\n",
        "                \"entity\": entity[\"label\"],\n",
        "                \"word\": entity[\"text\"],\n",
        "                \"start\": entity[\"start\"],\n",
        "                \"end\": entity[\"end\"],\n",
        "                \"score\": 0,\n",
        "            }\n",
        "            for entity in model.predict_entities(\n",
        "                text, labels, flat_ner=not nested_ner, threshold=threshold\n",
        "            )\n",
        "        ],\n",
        "    }\n",
        "    r[\"entities\"] = merge_entities(r[\"entities\"])\n",
        "    return transform_data(r)\n",
        "\n",
        "class AutoAnnotator:\n",
        "    def __init__(\n",
        "        self, model: int = \"knowledgator/gliner-multitask-large-v0.5\",\n",
        "        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        ) -> None:\n",
        "\n",
        "        self.model = GLiNER.from_pretrained(model).to(device)\n",
        "        self.annotated_data = []\n",
        "        self.stat = {\n",
        "            \"total\": None,\n",
        "            \"current\": -1\n",
        "        }\n",
        "\n",
        "    def auto_annotate(\n",
        "            self, data: List[str], labels: List[str],\n",
        "            prompt: Union[str, List[str]] = None, threshold: float = 0.5, nested_ner: bool = False\n",
        "            ) -> List[Dict]:\n",
        "        self.stat[\"total\"] = len(data)\n",
        "        self.stat[\"current\"] = -1  # Reset current progress\n",
        "        for text in data:\n",
        "            self.stat[\"current\"] += 1\n",
        "            if isinstance(prompt, list):\n",
        "                prompt_text = random.choice(prompt)\n",
        "            else:\n",
        "                prompt_text = prompt\n",
        "            text = f\"{prompt_text}\\n{text}\" if prompt_text else text\n",
        "\n",
        "            annotation = annotate_text(self.model, text, labels, threshold, nested_ner)\n",
        "\n",
        "            if not annotation[\"ner\"]:  # If no entities identified\n",
        "                annotation = {\"tokenized_text\": tokenize_text(text), \"ner\": [], \"validated\": False}\n",
        "\n",
        "            self.annotated_data.append(annotation)\n",
        "        return self.annotated_data\n",
        "\n",
        "# Define a global annotator\n",
        "annotator = None\n",
        "\n",
        "# Function to annotate data\n",
        "def annotate(model, labels, threshold, prompt):\n",
        "    global annotator\n",
        "    try:\n",
        "        labels = [label.strip() for label in labels.split(\",\")]\n",
        "        annotator = AutoAnnotator(model)\n",
        "        annotated_data = annotator.auto_annotate(sentences, labels, prompt, threshold)\n",
        "        with open(\"data/annotated_data.json\", \"wt\") as file:\n",
        "            json.dump(annotated_data, file)\n",
        "        return \"Successfully annotated and saved as data/annotated_data.json\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "\n",
        "# Gradio interface\n",
        "with gr.Blocks() as annotator_interface:\n",
        "    labels = gr.Textbox(label=\"Labels\", placeholder=\"Enter your comma-separated labels here\", scale=2)\n",
        "    model = gr.Dropdown(label=\"Choose the model which will be used for annotation\", choices=AVAILABLE_MODELS)\n",
        "    threshold = gr.Slider(0, 1, value=0.3, step=0.01, label=\"Threshold\", info=\"Lower the threshold to increase how many entities get predicted.\")\n",
        "    prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Enter your annotation prompt here\", scale=2)\n",
        "    submit_btn = gr.Button(\"Annotate data\")\n",
        "    output_info = gr.Textbox(label=\"Processing info:\")\n",
        "\n",
        "    submit_btn.click(fn=annotate, inputs=[model, labels, threshold, prompt], outputs=output_info)\n",
        "\n",
        "annotator_interface.launch(inline=True)\n"
      ],
      "metadata": {
        "id": "M16R6bFow5Lt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the cell above ☝️ to auto-annotate the dataset with one of the available GLiNER models**"
      ],
      "metadata": {
        "id": "mXJx3cMnLo17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title #If you have dataset, load it here\n",
        "import gradio as gr\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Ensure the /data directory exists\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Function to save the uploaded file\n",
        "def save_file(uploaded_file):\n",
        "    if uploaded_file is None:\n",
        "        return \"No file uploaded.\"\n",
        "\n",
        "    # Define the path where the file will be saved\n",
        "    save_path = os.path.join(\"data\", \"annotated_data.json\")\n",
        "\n",
        "    try:\n",
        "        # Save the file with the new name\n",
        "        shutil.copy(uploaded_file.name, save_path)\n",
        "        return f\"File saved to {save_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as loader:\n",
        "    gr.Markdown(\"# File Upload and Save Example\")\n",
        "\n",
        "    # File uploader component\n",
        "    file_uploader = gr.File(label=\"Upload your file here\")\n",
        "\n",
        "    # Button to trigger the file save function\n",
        "    save_button = gr.Button(\"Save File\")\n",
        "\n",
        "    # Output textbox to show the result\n",
        "    output = gr.Textbox(label=\"Result\")\n",
        "\n",
        "    # Link the button to the save_file function\n",
        "    save_button.click(fn=save_file, inputs=file_uploader, outputs=output)\n",
        "\n",
        "# Launch the interface\n",
        "loader.launch(share=True, inline=True)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B3-05cN-jWH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the cell above ☝️ to load already annotated dataset.**\n",
        "\n",
        "**⚡ Skip it if you auto-annotated dataset ⚡**"
      ],
      "metadata": {
        "id": "ynm09JX1OUAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset Viewer\n",
        "\n",
        "class DynamicDataset:\n",
        "    def __init__(\n",
        "            self, data: List[Dict[str, Union[List[Union[int, str]], bool]]]\n",
        "                 ) -> None:\n",
        "        self.data = data\n",
        "        self.data_len = len(self.data)\n",
        "        self.current = -1\n",
        "        for example in self.data:\n",
        "            if not \"validated\" in example.keys():\n",
        "                example[\"validated\"] = False\n",
        "\n",
        "    def next_example(self):\n",
        "        self.current += 1\n",
        "        if self.current > self.data_len-1:\n",
        "          self.current = self.data_len -1\n",
        "        elif self.current < 0:\n",
        "          self.current = 0\n",
        "\n",
        "    def previous_example(self):\n",
        "        self.current -= 1\n",
        "        if self.current > self.data_len-1:\n",
        "          self.current = self.data_len -1\n",
        "        elif self.current < 0:\n",
        "          self.current = 0\n",
        "\n",
        "    def example_by_id(self, id):\n",
        "        self.current = id\n",
        "        if self.current > self.data_len-1:\n",
        "          self.current = self.data_len -1\n",
        "        elif self.current < 0:\n",
        "          self.current = 0\n",
        "\n",
        "    def validate(self):\n",
        "        self.data[self.current][\"validated\"] = True\n",
        "\n",
        "    def load_current_example(self):\n",
        "        return self.data[self.current]\n",
        "\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenize the input text into a list of tokens.\"\"\"\n",
        "    return re.findall(r'\\w+(?:[-_]\\w+)*|\\S', text)\n",
        "\n",
        "\n",
        "def join_tokens(tokens):\n",
        "    # Joining tokens with space, but handling special characters correctly\n",
        "    text = \"\"\n",
        "    for token in tokens:\n",
        "        if token in {\",\", \".\", \"!\", \"?\", \":\", \";\", \"...\"}:\n",
        "            text = text.rstrip() + token\n",
        "        else:\n",
        "            text += \" \" + token\n",
        "    return text.strip()\n",
        "\n",
        "def prepare_for_highlight(data):\n",
        "    tokens = data[\"tokenized_text\"]\n",
        "    ner = data[\"ner\"]\n",
        "\n",
        "    highlighted_text = []\n",
        "    current_entity = None\n",
        "    entity_tokens = []\n",
        "    normal_tokens = []\n",
        "\n",
        "    for idx, token in enumerate(tokens):\n",
        "        # Check if the current token is the start of a new entity\n",
        "        if current_entity is None or idx > current_entity[1]:\n",
        "            if entity_tokens:\n",
        "                highlighted_text.append((\" \".join(entity_tokens), current_entity[2]))\n",
        "                entity_tokens = []\n",
        "            current_entity = next((entity for entity in ner if entity[0] == idx), None)\n",
        "\n",
        "        # If current token is part of an entity\n",
        "        if current_entity and current_entity[0] <= idx <= current_entity[1]:\n",
        "            if normal_tokens:\n",
        "                highlighted_text.append((\" \".join(normal_tokens), None))\n",
        "                normal_tokens = []\n",
        "            entity_tokens.append(token + \" \")\n",
        "        else:\n",
        "            if entity_tokens:\n",
        "                highlighted_text.append((\" \".join(entity_tokens), current_entity[2]))\n",
        "                entity_tokens = []\n",
        "            normal_tokens.append(token + \" \")\n",
        "\n",
        "    # Append any remaining tokens\n",
        "    if entity_tokens:\n",
        "        highlighted_text.append((\" \".join(entity_tokens), current_entity[2]))\n",
        "    if normal_tokens:\n",
        "        highlighted_text.append((\" \".join(normal_tokens), None))\n",
        "    # Clean up spaces before punctuation\n",
        "    cleaned_highlighted_text = []\n",
        "    for text, label in highlighted_text:\n",
        "        cleaned_text = re.sub(r'\\s(?=[,\\.!?…:;])', '', text)\n",
        "        cleaned_highlighted_text.append((cleaned_text, label))\n",
        "\n",
        "    return cleaned_highlighted_text\n",
        "\n",
        "def extract_tokens_and_labels(data: List[Dict[str, Union[str, None]]]) -> Dict[str, Union[List[str], List[Tuple[int, int, str]]]]:\n",
        "    tokens = []\n",
        "    ner = []\n",
        "\n",
        "    token_start_idx = 0\n",
        "\n",
        "    for entry in data:\n",
        "        char = entry['token']\n",
        "        label = entry['class_or_confidence']\n",
        "\n",
        "        # Tokenize the current text chunk\n",
        "        token_list = tokenize_text(char)\n",
        "\n",
        "        # Append tokens to the main tokens list\n",
        "        tokens.extend(token_list)\n",
        "\n",
        "        if label:\n",
        "            token_end_idx = token_start_idx + len(token_list) - 1\n",
        "            ner.append((token_start_idx, token_end_idx, label))\n",
        "\n",
        "        token_start_idx += len(token_list)\n",
        "\n",
        "    return tokens, ner\n",
        "\n",
        "def update_example(data):\n",
        "    global dynamic_dataset\n",
        "    tokens, ner = extract_tokens_and_labels(data)\n",
        "    dynamic_dataset.data[dynamic_dataset.current][\"tokenized_text\"] = tokens\n",
        "    dynamic_dataset.data[dynamic_dataset.current][\"ner\"] = ner\n",
        "    return prepare_for_highlight(dynamic_dataset.load_current_example())\n",
        "\n",
        "def validate_example():\n",
        "    global dynamic_dataset\n",
        "    dynamic_dataset.data[dynamic_dataset.current][\"validated\"] = True\n",
        "    return [(\"The example was validated!\", None)]\n",
        "\n",
        "def next_example():\n",
        "    dynamic_dataset.next_example()\n",
        "    return prepare_for_highlight(dynamic_dataset.load_current_example()), dynamic_dataset.current\n",
        "\n",
        "def previous_example():\n",
        "    dynamic_dataset.previous_example()\n",
        "    return prepare_for_highlight(dynamic_dataset.load_current_example()), dynamic_dataset.current\n",
        "\n",
        "def save_dataset(inp):\n",
        "  with open(\"data/annotated_data.json\", \"wt\") as file:\n",
        "    json.dump(dynamic_dataset.data, file)\n",
        "  return [(\"The validates dataset was saved as data/annotated_data.json\", None)]\n",
        "\n",
        "with open(\"data/annotated_data.json\", 'rt') as dataset:\n",
        "  ANNOTATED_DATA = json.load(dataset)\n",
        "dynamic_dataset = DynamicDataset(ANNOTATED_DATA)\n",
        "DATASET_LEN = len(dynamic_dataset.data)\n",
        "\n",
        "with gr.Blocks() as dataset_viewer:\n",
        "    bar = gr.Slider(minimum=0, maximum=DATASET_LEN -1, step=1, label=\"Progress\", interactive=False)\n",
        "    with gr.Row():\n",
        "        previous_btn = gr.Button(\"Previous example\")\n",
        "        apply_btn = gr.Button(\"Apply changes\")\n",
        "        next_btn = gr.Button(\"Next example\")\n",
        "    validate_btn = gr.Button(\"Validate\")\n",
        "    save_btn = gr.Button(\"Save validated dataset\")\n",
        "\n",
        "    inp_box = gr.HighlightedText(value=None, interactive=True)\n",
        "    apply_btn.click(fn=update_example, inputs=inp_box, outputs=inp_box)\n",
        "    save_btn.click(fn=save_dataset, inputs=inp_box, outputs=inp_box)\n",
        "    validate_btn.click(fn=validate_example, inputs=None, outputs=inp_box)\n",
        "    next_btn.click(fn=next_example, inputs=None, outputs=[inp_box,bar])\n",
        "    previous_btn.click(fn=previous_example, inputs=None, outputs=[inp_box,bar])\n",
        "\n",
        "dataset_viewer.launch(share=True, inline=True)"
      ],
      "metadata": {
        "id": "5DOfHuPgxB8i",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click `Next example` to access the first dataset item. Click `Apply changes` to save your annotation.\n",
        "\n",
        "\\\n",
        "\n",
        "To annotate an entity just highlight a text and write an appropriate label name\n",
        "\n",
        "\\\n",
        "⚡ Don't forget to `Save validated dataset`"
      ],
      "metadata": {
        "id": "5l8NOxAETED6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train the model\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "def load_and_prepare_data(train_path, split_ratio):\n",
        "    if not os.path.exists(train_path):\n",
        "        raise FileNotFoundError(f\"The file {train_path} does not exist.\")\n",
        "\n",
        "    with open(train_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    random.seed(42)\n",
        "    random.shuffle(data)\n",
        "    train_data = data[:int(len(data) * split_ratio)]\n",
        "    test_data = data[int(len(data) * split_ratio):]\n",
        "    return train_data, test_data\n",
        "\n",
        "def create_models_directory():\n",
        "    if not os.path.exists(\"models\"):\n",
        "        os.makedirs(\"models\")\n",
        "\n",
        "def train_model(model_name, custom_model_name, train_path, split_ratio, learning_rate, weight_decay, batch_size, epochs, compile_model):\n",
        "    global train_data, train_data\n",
        "    create_models_directory()\n",
        "\n",
        "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"Loading model...\")\n",
        "    model = GLiNER.from_pretrained(model_name)\n",
        "\n",
        "    print(\"Loading and preparing data...\")\n",
        "    train_data, test_data = load_and_prepare_data(train_path, split_ratio)\n",
        "\n",
        "    with open(\"data/test.json\", \"wt\") as file:\n",
        "      json.dump(test_data, file)\n",
        "    print(f\"Training data size: {len(train_data)}, Testing data size: {len(test_data)}\")\n",
        "\n",
        "    train_dataset = GLiNERDataset(train_data, model.config, data_processor=model.data_processor)\n",
        "    test_dataset = GLiNERDataset(test_data, model.config, data_processor=model.data_processor)\n",
        "    data_collator = DataCollatorWithPadding(model.config)\n",
        "\n",
        "    if compile_model:\n",
        "        print(\"Compiling model for faster training...\")\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "        model.to(device)\n",
        "        model.compile_for_training()\n",
        "    else:\n",
        "        model.to(device)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"models\",\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        others_lr=learning_rate,\n",
        "        others_weight_decay=weight_decay,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        warmup_ratio=0.1,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_steps=1000,\n",
        "        save_total_limit=10,\n",
        "        dataloader_num_workers=8,\n",
        "        use_cpu=(device == torch.device('cpu')),\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        tokenizer=model.data_processor.transformer_tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    model.save_pretrained(f\"models/{custom_model_name}\")\n",
        "\n",
        "    return \"Training completed successfully.\"\n",
        "\n",
        "# Gradio interface\n",
        "def gradio_train(model_name, custom_model_name, train_path, split_ratio, learning_rate, weight_decay, batch_size, epochs, compile_model):\n",
        "    train_path = os.path.join(\"data\", train_path)\n",
        "    try:\n",
        "        return train_model(model_name, custom_model_name, train_path, split_ratio, learning_rate, weight_decay, batch_size, epochs, compile_model)\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "with gr.Blocks() as trainer_interface:\n",
        "    gr.Markdown(\"# GLiNER Training Interface\")\n",
        "    train_path = os.listdir(\"data\")\n",
        "    #local_models = [f\"models/{local_model}\" for local_model in os.listdir(\"models\")]\n",
        "    with gr.Row():\n",
        "      model_name = gr.Dropdown(label=\"Choose the parent model\", choices=AVAILABLE_MODELS, value=\"knowledgator/gliner-multitask-large-v0.5\")\n",
        "      custom_model_name = gr.Textbox(label=\"The name of your custom model\", placeholder=\"Enter the name of your new model\")\n",
        "      train_path = gr.Dropdown(label= \"Choose the dataset\",choices=train_path, value=\"annotated_data.json\")\n",
        "      split_ratio = gr.Slider(label=\"Train/Test Split Ratio\", minimum=0.1, maximum=0.9, step=0.1, value=0.9)\n",
        "    with gr.Row():\n",
        "      learning_rate = gr.Slider(label=\"Learning Rate\", minimum=1e-6, maximum=1e-4, step=1e-6, value=5e-6)\n",
        "      weight_decay = gr.Slider(label=\"Weight Decay\", minimum=0, maximum=0.1, step=0.01, value=0.01)\n",
        "      batch_size = gr.Slider(label=\"Batch Size\", minimum=1, maximum=128, step=1, value=8)\n",
        "      epochs = gr.Slider(label=\"Number of Epochs\", minimum=1, maximum=10, step=1, value=1)\n",
        "    compile_model = gr.Checkbox(label=\"Compile Model for Faster Training\", value=False)\n",
        "    train_btn = gr.Button(\"Start Training\")\n",
        "\n",
        "    output_info = gr.Textbox(label=\"Training Info\")\n",
        "\n",
        "    train_btn.click(fn=gradio_train, inputs=[model_name, custom_model_name, train_path, split_ratio, learning_rate, weight_decay, batch_size, epochs, compile_model], outputs=output_info)\n",
        "\n",
        "trainer_interface.launch(inline=True)\n"
      ],
      "metadata": {
        "id": "U0a6CQvIP_Bm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choose a model and set training parameters for your needs**"
      ],
      "metadata": {
        "id": "k1WaKJOPVYDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fast Mertics\n",
        "\n",
        "# Load the test.json file\n",
        "with open('data/test.json', 'r') as file:\n",
        "    test_data = json.load(file)\n",
        "\n",
        "with open('data/annotated_data.json', 'r') as file:\n",
        "    annotated_data = json.load(file)\n",
        "\n",
        "# Extract all labels from each example\n",
        "all_labels = []\n",
        "for example in annotated_data:\n",
        "    ner_data = example.get(\"ner\", [])\n",
        "    for entity in ner_data:\n",
        "        label = entity[2]  # Assuming the label is the third element in the entity list\n",
        "        if label not in all_labels:\n",
        "            all_labels.append(label)\n",
        "\n",
        "def evaluate_model(model_name):\n",
        "    model_path = f\"models/{model_name}\"\n",
        "    model = GLiNER.from_pretrained(model_path, load_tokenizer=True, local_files_only=True)\n",
        "\n",
        "    def get_for_one_path(test_dataset, entity_types):\n",
        "        # evaluate the model\n",
        "        results, f1 = model.evaluate(test_dataset, flat_ner=True, threshold=0.5, batch_size=12, entity_types=entity_types)\n",
        "        return results, f1\n",
        "\n",
        "    results, f1 = get_for_one_path(test_data, all_labels)\n",
        "    output_info = f\"F1 Score: {f1:.2f}\" + \"\\n\" + results\n",
        "    return output_info\n",
        "\n",
        "with gr.Blocks() as evaluation_interface:\n",
        "    gr.Markdown(\"# GLiNER Evaluation Interface\")\n",
        "    models = os.listdir(\"models\")\n",
        "    model_name = gr.Dropdown(label=\"Choose the model\", choices=models, value=models[0])\n",
        "\n",
        "    evaluate_btn = gr.Button(\"Evaluate Model\")\n",
        "    output_info = gr.Textbox(label=\"Evaluation Info\")\n",
        "\n",
        "    evaluate_btn.click(fn=evaluate_model, inputs=model_name, outputs=output_info)\n",
        "\n",
        "# Suppress all prints\n",
        "evaluation_interface.launch()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rwxa-Eg671-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NER Inferance\n",
        "\n",
        "class Model:\n",
        "  def __init__(self) -> None:\n",
        "      self.previous_path = None\n",
        "      self.path = None\n",
        "      self.model = None\n",
        "  def get_model(self, path):\n",
        "      self.previous_path = None\n",
        "      self.path = path\n",
        "      if self.path != self.previous_path:\n",
        "          self.model = GLiNER.from_pretrained(f\"models/{self.path}\", load_tokenizer=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.previous_path = self.path\n",
        "      return self.model\n",
        "\n",
        "model_generator = Model()\n",
        "\n",
        "text1 = \"\"\"\n",
        "\"I recently purchased the Sony WH-1000XM4 Wireless Noise-Canceling Headphones from Amazon and I must say, I'm thoroughly impressed. The package arrived in New York within 2 days, thanks to Amazon Prime's expedited shipping.\n",
        "\n",
        "The headphones themselves are remarkable. The noise-canceling feature works like a charm in the bustling city environment, and the 30-hour battery life means I don't have to charge them every day. Connecting them to my Samsung Galaxy S21 was a breeze, and the sound quality is second to none.\n",
        "\n",
        "I also appreciated the customer service from Amazon when I had a question about the warranty. They responded within an hour and provided all the information I needed.\n",
        "\n",
        "However, the headphones did not come with a hard case, which was listed in the product description. I contacted Amazon, and they offered a 10% discount on my next purchase as an apology.\n",
        "\n",
        "Overall, I'd give these headphones a 4.5/5 rating and highly recommend them to anyone looking for top-notch quality in both product and service.\"\"\"\n",
        "\n",
        "\n",
        "text3 = \"\"\"\n",
        "Several studies have reported its pharmacological activities, including anti-inflammatory, antimicrobial, and antitumoral effects.\n",
        "The effect of E-anethole was studied in the osteosarcoma MG-63 cell line, and the antiproliferative activity was evaluated by an MTT assay.\n",
        "It showed a GI50 value of 60.25 μM with apoptosis induction through the mitochondrial-mediated pathway. Additionally, it induced cell cycle arrest at the G0/G1 phase, up-regulated the expression of p53, caspase-3, and caspase-9, and down-regulated Bcl-xL expression.\n",
        "Moreover, the antitumoral activity of anethole was assessed against oral tumor Ca9-22 cells, and the cytotoxic effects were evaluated by MTT and LDH assays.\n",
        "It demonstrated a LD50 value of 8 μM, and cellular proliferation was 42.7% and 5.2% at anethole concentrations of 3 μM and 30 μM, respectively.\n",
        "It was reported that it could selectively and in a dose-dependent manner decrease cell proliferation and induce apoptosis, as well as induce autophagy, decrease ROS production, and increase glutathione activity. The cytotoxic effect was mediated through NF-kB, MAP kinases, Wnt, caspase-3 and -9, and PARP1 pathways. Additionally, treatment with anethole inhibited cyclin D1 oncogene expression, increased cyclin-dependent kinase inhibitor p21WAF1, up-regulated p53 expression, and inhibited the EMT markers.\n",
        "\"\"\"\n",
        "\n",
        "text5 = \"\"\"\n",
        "Dr. Paul Hammond, a renowned neurologist at Johns Hopkins University, has recently published a paper in the prestigious journal \"Nature Neuroscience\". His research focuses on a rare genetic mutation, found in less than 0.01% of the population, that appears to prevent the development of Alzheimer's disease. Collaborating with researchers at the University of California, San Francisco, the team is now working to understand the mechanism by which this mutation confers its protective effect. Funded by the National Institutes of Health, their research could potentially open new avenues for Alzheimer's treatment.\n",
        "\"\"\"\n",
        "\n",
        "ner_examples = [\n",
        "    [\n",
        "        text5,\n",
        "        \"neurologist, scientist, gene, disease, biological process, city, journal, university\",\n",
        "        0.5,\n",
        "        False\n",
        "    ],\n",
        "    [\n",
        "        text1,\n",
        "        \"product, brand, location, features, rating\",\n",
        "        0.5,\n",
        "        False\n",
        "    ],\n",
        "    [\n",
        "        text3,\n",
        "        \"cell line, protein, metric, substance\",\n",
        "        0.5,\n",
        "        False\n",
        "    ]]\n",
        "\n",
        "def merge_entities(entities):\n",
        "    if not entities:\n",
        "        return []\n",
        "    merged = []\n",
        "    current = entities[0]\n",
        "    for next_entity in entities[1:]:\n",
        "        if next_entity['entity'] == current['entity'] and (next_entity['start'] == current['end'] + 1 or next_entity['start'] == current['end']):\n",
        "            current['word'] += ' ' + next_entity['word']\n",
        "            current['end'] = next_entity['end']\n",
        "        else:\n",
        "            merged.append(current)\n",
        "            current = next_entity\n",
        "    merged.append(current)\n",
        "    return merged\n",
        "\n",
        "def process(\n",
        "    model_name, text, labels: str, threshold: float, nested_ner: bool\n",
        ") -> Dict[str, Union[str, int, float]]:\n",
        "    model = model_generator.get_model(model_name)\n",
        "    labels = [label.strip() for label in labels.split(\",\")]\n",
        "    r = {\n",
        "        \"text\": text,\n",
        "        \"entities\": [\n",
        "            {\n",
        "                \"entity\": entity[\"label\"],\n",
        "                \"word\": entity[\"text\"],\n",
        "                \"start\": entity[\"start\"],\n",
        "                \"end\": entity[\"end\"],\n",
        "                \"score\": 0,\n",
        "            }\n",
        "            for entity in model.predict_entities(\n",
        "                text, labels, flat_ner=not nested_ner, threshold=threshold\n",
        "            )\n",
        "        ],\n",
        "    }\n",
        "    r[\"entities\"] =  merge_entities(r[\"entities\"])\n",
        "    return r\n",
        "\n",
        "#model = GLiNER.from_pretrained(f\"{MODEL_NAME}\", load_tokenizer=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with gr.Blocks(title=\"NER Task\") as ner_interface:\n",
        "    models = os.listdir(\"models\")\n",
        "    if not models:\n",
        "        print(\"No models found in the 'models' directory.\")\n",
        "    model_name = gr.Dropdown(label=\"Model Name\", choices=models)\n",
        "    input_text = gr.Textbox(label=\"Text input\", placeholder=\"Enter your text here\")\n",
        "    labels = gr.Textbox(label=\"Labels\", placeholder=\"Enter your labels here (comma separated)\", scale=2)\n",
        "    threshold = gr.Slider(0, 1, value=0.3, step=0.01, label=\"Threshold\", info=\"Lower the threshold to increase how many entities get predicted.\")\n",
        "    nested_ner = gr.Checkbox(label=\"Nested NER\", info=\"Allow for nested NER?\")\n",
        "    output = gr.HighlightedText(label=\"Predicted Entities\")\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    examples = gr.Examples(\n",
        "        ner_examples,\n",
        "        fn=process,\n",
        "        inputs=[input_text, labels, threshold, nested_ner],\n",
        "        outputs=output,\n",
        "        cache_examples=False\n",
        "    )\n",
        "    theme=gr.themes.Base()\n",
        "\n",
        "    input_text.submit(fn=process, inputs=[model_name, input_text, labels, threshold, nested_ner], outputs=output)\n",
        "    labels.submit(fn=process, inputs=[model_name, input_text, labels, threshold, nested_ner], outputs=output)\n",
        "    threshold.release(fn=process, inputs=[model_name, input_text, labels, threshold, nested_ner], outputs=output)\n",
        "    submit_btn.click(fn=process, inputs=[model_name, input_text, labels, threshold, nested_ner], outputs=output)\n",
        "    nested_ner.change(fn=process, inputs=[model_name, input_text, labels, threshold, nested_ner], outputs=output)\n",
        "\n",
        "ner_interface.launch()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VWPkUahzN5fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload model to Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def zip_directory(model_name):\n",
        "    model_path = f\"models/{model_name}\"\n",
        "    zip_path = f\"{model_path}.zip\"\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for root, dirs, files in os.walk(model_path):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, start=model_path)\n",
        "                    zipf.write(file_path, arcname)\n",
        "        return zip_path\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def upload_to_drive(zip_path, drive_folder='My Drive'):\n",
        "    if zip_path and os.path.exists(zip_path):\n",
        "        destination_dir = f'/content/drive/{drive_folder}'\n",
        "        os.makedirs(destination_dir, exist_ok=True)\n",
        "        destination = f'{destination_dir}/{os.path.basename(zip_path)}'\n",
        "        shutil.move(zip_path, destination)\n",
        "        return f\"File uploaded to {destination}\"\n",
        "    else:\n",
        "        return \"Zip file not found.\"\n",
        "\n",
        "def zip_and_upload(model_name, drive_path):\n",
        "    zip_path = zip_directory(model_name)\n",
        "    if zip_path:\n",
        "        upload_message = upload_to_drive(zip_path, drive_folder=drive_path)\n",
        "        return f\"Directory '{model_name}' zipped successfully as '{zip_path}'. {upload_message}\"\n",
        "    else:\n",
        "        return f\"Directory '{model_name}' not found.\"\n",
        "\n",
        "with gr.Blocks() as to_drive:\n",
        "    gr.Markdown(\"# GLiNER Model Zipper and Uploader\")\n",
        "\n",
        "    models = os.listdir(\"models\")\n",
        "    model_name = gr.Dropdown(label=\"Choose the model\", choices=models, value=models[0])\n",
        "    drive_path = gr.Textbox(label=\"Google Drive Path\", placeholder=\"Enter the path on Google Drive (e.g., 'My Drive/Models')\", value='My Drive/Models')\n",
        "    upload_btn = gr.Button(\"Zip and Upload Model\")\n",
        "    output_info = gr.Textbox(label=\"Output Info\")\n",
        "    upload_btn.click(fn=zip_and_upload, inputs=[model_name, drive_path], outputs=output_info)\n",
        "\n",
        "# Launch the Gradio interface\n",
        "to_drive.launch(inline=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MqNq2pjJ3tLN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}